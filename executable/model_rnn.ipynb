{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model is a 3-layer LSTM model with skip connection between layers (like DenseNet). I found that RMSprop (with clipping gradients) is more stable than Adam. Recent research papers show that SGD gives the best results. However, due to limited time, I have not tried to optimize the model using SGD yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, I skipped the first three months and take the loss of (randomly selected) 385 days to do back-propagation. With `batch_size=256` and 6 epochs, it took me an hour to train the model (quite fast to obtain a reasonable result)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.507706Z",
     "start_time": "2017-09-12T19:44:36.064138Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.539778Z",
     "start_time": "2017-09-12T19:44:36.509012Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_2.csv',\n",
    "    'key_file': 'key_2.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'train_len': 385,\n",
    "    'train_skip': 91,\n",
    "    'val_len': 64,\n",
    "    'offset': 803,\n",
    "    'batch_size': 256,\n",
    "    'hidden_size': 256,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': False,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'forecast': True,\n",
    "    'cuda': True,\n",
    "    'seed': 20170913,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.620245Z",
     "start_time": "2017-09-12T19:44:36.540870Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size)\n",
    "        self.linear = nn.Linear(3*hidden_size+1, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, future=0):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([x_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "            \n",
    "        for i in range(future):\n",
    "            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([o_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([o_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.691044Z",
     "start_time": "2017-09-12T19:44:36.621493Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    y_pred = np.around(y_pred)\n",
    "    denominator = y_true + y_pred\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0\n",
    "    return 200 * np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.765495Z",
     "start_time": "2017-09-12T19:44:36.695625Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    raw_data_file = os.path.join(args.intermediate_path, 'raw_data.pkl')\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        raw_data = data_df.values.copy()\n",
    "        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n",
    "            method='bfill', axis=1)\n",
    "        data = np.nan_to_num(data_df.values.astype('float32'))\n",
    "        data = np.log1p(data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(raw_data_file, 'wb') as f:\n",
    "            pickle.dump(raw_data, f)\n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "    else:\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            raw_data = pickle.load(f)\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "    return raw_data, scaled_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.834615Z",
     "start_time": "2017-09-12T19:44:36.767410Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(raw_data, scaled_data, scaler, model, criterion, optimizer):\n",
    "    p = np.random.permutation(scaled_data.shape[0])\n",
    "    inverse_p = np.argsort(p)\n",
    "    \n",
    "    input_tensor = torch.from_numpy(\n",
    "        scaled_data[p, :(args.offset-1)]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(\n",
    "        scaled_data[p, 1:args.offset]).unsqueeze(2)\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "#    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        \n",
    "#        output = model(inputt, future=args.val_len)\n",
    "        output = model(inputt)\n",
    "        pos = np.random.randint(args.train_skip,\n",
    "                                inputt.size(1)-args.train_len+1)\n",
    "        loss = criterion(output[:, pos:pos+args.train_len],\n",
    "                         target[:, pos:pos+args.train_len])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), 3, 'inf')\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "#        val_output_list.append(output[:, -args.val_len:]\n",
    "#                               .data.squeeze(2).cpu().numpy())\n",
    "        \n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train loss: {:.4f}\".format(\n",
    "                      time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "#    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "#    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "#           np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "#    prediction = np.clip(np.exp(prediction)-1, 0, None)\n",
    "#    var_target = raw_data[:, args.offset:args.offset+args.val_len]\n",
    "    \n",
    "    train_loss /= scaled_data.shape[0]\n",
    "#    val_loss = smape(prediction, var_target)\n",
    "    val_loss = 0\n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n",
    "          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss, val_loss))\n",
    "    print(\"=\"*10)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.899012Z",
     "start_time": "2017-09-12T19:44:36.835860Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast(raw_data, scaled_data, scaler, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data[:,\n",
    "            :args.offset]).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, 128)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        output = model(inputt, args.val_len)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.val_len:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "\n",
    "    prediction = np.clip(np.exp(prediction) - 1, 0, None)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:36.951749Z",
     "start_time": "2017-09-12T19:44:36.900198Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_{}_epoch{}_loss{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:37.408805Z",
     "start_time": "2017-09-12T19:44:36.956297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data, scaled_data, scaler = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:38.527818Z",
     "start_time": "2017-09-12T19:44:37.410162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:38.531189Z",
     "start_time": "2017-09-12T19:44:38.528965Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:44:38.582262Z",
     "start_time": "2017-09-12T19:44:38.532253Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, 7):\n",
    "        scheduler.step()\n",
    "        print(\"=> EPOCH {} with lr {}\".format(epoch, scheduler.get_lr()))\n",
    "        val_loss = train(raw_data, scaled_data, scaler,\n",
    "                         model, criterion, optimizer)\n",
    "        save_model(model, epoch, val_loss)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:53:30.116279Z",
     "start_time": "2017-09-12T19:44:38.583358Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.forecast:\n",
    "    prediction = forecast(raw_data, scaled_data, scaler, model)\n",
    "#    print(\"SMAPE: {}\".format(smape(prediction, raw_data[:,\n",
    "#        args.offset:args.offset+args.val_len])))\n",
    "    with open(os.path.join(args.intermediate_path,\n",
    "                           \"pred_rnn.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydata)",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
